{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Spark",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pyspark as ps\n",
    "print(ps.__version__)"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lower\n",
    "spark = SparkSession.builder.appName('ReadCSV').getOrCreate()\n",
    "\n",
    "csv_path = \"./assets/complete.csv\"\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "\n",
    "df.show()"
   ],
   "id": "ff21bbb0328d2978",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.select(\"Name of State / UT\").show()",
   "id": "a29d2ecdadc3c954",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.showing lowercase of state name",
   "id": "724277570bc75962"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_lowercase = df.withColumn(\"Name of State / UT\", lower(df[\"Name of State / UT\"]))\n",
    "df_lowercase.select(\"Name of State / UT\").show(truncate=False)\n",
    "df_lowercase.show(truncate=False,n=df_lowercase.count())"
   ],
   "id": "4520bac6e06072be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(df.columns)",
   "id": "42eb9471c4d5df60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.Day had high covid cases",
   "id": "8755698423f66de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df_high_cases = df.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "\n",
    "#sum of new cases\n",
    "new_cases_by_day = df_high_cases.groupBy(\"Date\").agg(sum(\"New cases\").alias(\"total_new_cases\"))\n",
    "\n",
    "#finding date\n",
    "day_with_highest_new_cases = new_cases_by_day.orderBy(col(\"total_new_cases\").desc()).limit(1)\n",
    "\n",
    "# Show the result\n",
    "print(\"Day with more new cases:\")\n",
    "# new_cases_by_day.show(n=new_cases_by_day.count())\n",
    "day_with_highest_new_cases.show(truncate=False)"
   ],
   "id": "ec016fd43e7b2b0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3. 2nd State with no of cases",
   "id": "42e580fa26df3025"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "cases_by_state = df.groupBy(\"Name of State / UT\").agg(sum(\"Total Confirmed cases\").alias(\"total_confirmed_cases\"))\n",
    "\n",
    "# state with 2nd largest no of total cases\n",
    "states_sorted = cases_by_state.orderBy(col(\"total_confirmed_cases\").desc())\n",
    "state_with_second_largest_cases = states_sorted.limit(2).collect()[1]\n",
    "\n",
    "print(\"State with 2nd largest no of cases:\")\n",
    "print(f\"State: {state_with_second_largest_cases['Name of State / UT']}, Total Confirmed Cases: {state_with_second_largest_cases['total_confirmed_cases']}\")"
   ],
   "id": "ed8facc044b7c487",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4. UT with less deaths",
   "id": "7d1068642bb71fa9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "union_territories = ['Delhi', 'Chandigarh', 'Puducherry', 'Ladakh', 'Lakshadweep']\n",
    "df_ut = df.filter(col(\"Name of State / UT\").isin(union_territories))\n",
    "\n",
    "# total deaths\n",
    "deaths_by_ut = df_ut.groupBy(\"Name of State / UT\").agg(sum(\"Death\").alias(\"total_deaths\"))\n",
    "\n",
    "# filter\n",
    "ut_with_least_deaths = deaths_by_ut.orderBy(col(\"total_deaths\").asc()).limit(1)\n",
    "\n",
    "\n",
    "print(\"UT with least no of deaths:\")\n",
    "deaths_by_ut.show(truncate=False)\n",
    "ut_with_least_deaths.show(truncate=False)"
   ],
   "id": "8ef620e3ee04742",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5.state has lowest death to total confirmed ratio",
   "id": "c30cbccafb95bb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "aggregated_data = df.groupBy(\"Name of State / UT\").agg(\n",
    "    sum(\"Death\").alias(\"total_deaths\"),\n",
    "    sum(\"Total Confirmed cases\").alias(\"total_confirmed_cases\")\n",
    ")\n",
    "\n",
    "# Calculate the death-to-confirmed cases ratio\n",
    "# Use a conditional expression to avoid division by zero\n",
    "\n",
    "\n",
    "aggregated_data = aggregated_data.withColumn(\n",
    "    \"death_to_confirmed_ratio\",\n",
    "    expr(\"total_deaths / total_confirmed_cases\")\n",
    ")\n",
    "\n",
    "# Find the state with the lowest death-to-confirmed cases ratio\n",
    "state_with_lowest_ratio = aggregated_data.orderBy(col(\"death_to_confirmed_ratio\").asc()).limit(1)\n",
    "\n",
    "# Show the result\n",
    "print(\"State with the lowest death-to-confirmed cases ratio:\")\n",
    "# aggregated_data.show()\n",
    "state_with_lowest_ratio.show(truncate=False)"
   ],
   "id": "eb49259bf826cf1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6.month with more cases",
   "id": "a46c2953c02dca7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import month\n",
    "\n",
    "df_date = df.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "\n",
    "df_monthly = df_date.groupBy(month(\"Date\").alias(\"Month\")).agg(sum(\"New recovered\").alias(\"total_recovered_cases\"))\n",
    "\n",
    "# month\n",
    "month_with_most_recovered = df_monthly.orderBy(col(\"total_recovered_cases\").desc()).limit(1)\n",
    "\n",
    "\n",
    "month_names = {\n",
    "    1: 'January', 2: 'February', 3: 'March', 4: 'April', 5: 'May', 6: 'June',\n",
    "    7: 'July', 8: 'August', 9: 'September', 10: 'October', 11: 'November', 12: 'December'\n",
    "}\n",
    "\n",
    "# Print name\n",
    "result = month_with_most_recovered.collect()[0]\n",
    "month_number = result[\"Month\"]\n",
    "month_name = month_names[month_number]\n",
    "\n",
    "print(f\"Month with the highest number of recovered cases: {month_name}\")"
   ],
   "id": "75be4f21c34ae257",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
